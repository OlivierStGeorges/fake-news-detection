{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN, LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/train.csv')\n",
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20761, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indexes = dataset.loc[pd.isna(dataset[\"text\"]), :].index\n",
    "dataset = dataset.drop(nan_indexes)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(142 in nan_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.text\n",
    "y = dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  U.N. Secretary General Complains That The ‘Masses Have Rejected Globalism’ In Favor Of Nationalism Antonio Guterres, elected in October to take over as U.N. secretary general next year, told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas.   22, 2016 The incoming head of the United Nations warned on Tuesday that ‘losers of globalization’ in rich countries have felt ignored by establishment politicians, prompting them to turn to nationalist agendas, as in the U.S. election and Brexit referendum. “Therefore wait ye upon me, saith the LORD, until the day that I rise up to the prey: for my determination is to gather the nations, that I may assemble the kingdoms, to pour upon them mine indignation, even all my fierce anger: for all the earth shall be devoured with the fire of my jealousy.” Zephaniah 3:8 (KJV) EDITOR’S NOTE: The Bible clearly says that God’s desire is to “gather all the nations of the world together”, in order to “pour His fury” upon them. The United Nations, something unique in world history, has been created by the will of God, and things are going to end exactly how the Bible says they will end. All Muslims will be driven out of Israel as prophesied in Zechariah 14:21 (KJV), Israel will expand its borders to cover the size of the original land grant to Abraham , and Jesus will rule the world from Jerusalem . And that will be the “new” world order. Antonio Guterres, elected in October to take over as U.N. secretary general next year , told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas. Antonio Guterres formally elected as UN chief: Europe has struggled to handle a huge influx of refugees, many of whom displaced by the war in Syria. The United States has accepted only a very small number of refugees and may take in even fewer next year. “In 2016, we have witnessed a dramatic deterioration of that international protection regime (for refugees),” Guterres said. “This example started in the developed world, it started essentially in Europe, it is spreading now like a virus into other parts of the world.” Guterres, who was U.N. High Commissioner for Refugees until last year, linked the growing resistance to accepting refugees to wider concerns about globalism. “I don’t think we can look strictly at the refugee issue, I think the problem is a broader problem,” he told the conference on Europe’s refugee crisis. There was a consensus in the mid-1990s that globalization would benefit all, he said. “But a lot of people were left behind … In the developed world, (there are) those who have been losers in globalization,” he said. “The recent analysis of the rust belt in the United States, I think, is a clear demonstration of that, when we speak about the elections.” Donald Trump won this month’s election in the United States in part thanks to support from voters who have seen their jobs lost to countries with cheaper labor. “So globalization has not been as successful as we had hoped and lots of people became not only angry with it, but feeling that political establishments and international organizations are not paying attention, were not taking care (of them),” he said. This led to what he called “a kind of evolution” in which anti-establishment parties now tended to win elections and referendums tended to attract majorities against whatever was put to a vote. source SHARE THIS ARTICLE  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773.6980395934685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_article_length = 0\n",
    "\n",
    "for i in range(20761):\n",
    "    if i in nan_indexes:\n",
    "        continue\n",
    "    mean_article_length += len(X[i].split(' '))\n",
    "\n",
    "mean_article_length /= 20761\n",
    "\n",
    "mean_article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 1000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(128,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras as K\n",
    "\n",
    "def train_model(model_id, model, optimizer, epochs, X_train, y_train, validation_split, batch_size):\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('NNs/model-{epoch:03d}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    tb_callback = K.callbacks.TensorBoard(log_dir='./logs/nn_' + str(model_id))\n",
    "\n",
    "    hist = model.fit(X_train, y_train, validation_split=validation_split, epochs=epochs, batch_size=batch_size, callbacks=[tb_callback, checkpoint])\n",
    "    \n",
    "    return hist\n",
    "\n",
    "#hist = train_model(...)\n",
    "#best_epoch = max(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ens/AM91520/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/ens/AM91520/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 137,889\n",
      "Trainable params: 137,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /home/ens/AM91520/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/30\n",
      "16608/16608 [==============================] - 60s 4ms/step - loss: 0.3906 - acc: 0.8309 - val_loss: 0.1852 - val_acc: 0.9302\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.18519, saving model to NNs/model-001.h5\n",
      "Epoch 2/30\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1702 - acc: 0.9382 - val_loss: 0.1586 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.18519 to 0.15856, saving model to NNs/model-002.h5\n",
      "Epoch 3/30\n",
      "16608/16608 [==============================] - 63s 4ms/step - loss: 0.1191 - acc: 0.9594 - val_loss: 0.1929 - val_acc: 0.9307\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.15856\n",
      "Epoch 4/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1026 - acc: 0.9642 - val_loss: 0.1697 - val_acc: 0.9437\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.15856\n",
      "Epoch 5/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1086 - acc: 0.9604 - val_loss: 0.3614 - val_acc: 0.8353\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.15856\n",
      "Epoch 6/30\n",
      "16608/16608 [==============================] - 63s 4ms/step - loss: 0.2224 - acc: 0.9078 - val_loss: 0.2412 - val_acc: 0.9102\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.15856\n",
      "Epoch 7/30\n",
      "16608/16608 [==============================] - 66s 4ms/step - loss: 0.1043 - acc: 0.9629 - val_loss: 0.1537 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.15856 to 0.15372, saving model to NNs/model-007.h5\n",
      "Epoch 8/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.0665 - acc: 0.9778 - val_loss: 0.1609 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15372\n",
      "Epoch 9/30\n",
      "16608/16608 [==============================] - 65s 4ms/step - loss: 0.0731 - acc: 0.9752 - val_loss: 0.1975 - val_acc: 0.9304\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15372\n",
      "Epoch 10/30\n",
      "16608/16608 [==============================] - 66s 4ms/step - loss: 0.0533 - acc: 0.9828 - val_loss: 0.2034 - val_acc: 0.9461\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15372\n",
      "Epoch 11/30\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0715 - acc: 0.9755 - val_loss: 0.2120 - val_acc: 0.9338\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.15372\n",
      "Epoch 12/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.2988 - acc: 0.8724 - val_loss: 0.3630 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15372\n",
      "Epoch 13/30\n",
      "16608/16608 [==============================] - 55s 3ms/step - loss: 0.1288 - acc: 0.9523 - val_loss: 0.2359 - val_acc: 0.9280\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.15372\n",
      "Epoch 14/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.1116 - acc: 0.9594 - val_loss: 0.2452 - val_acc: 0.9278\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.15372\n",
      "Epoch 15/30\n",
      "16608/16608 [==============================] - 60s 4ms/step - loss: 0.0952 - acc: 0.9703 - val_loss: 0.1965 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.15372\n",
      "Epoch 16/30\n",
      "16608/16608 [==============================] - 61s 4ms/step - loss: 0.1417 - acc: 0.9506 - val_loss: 0.2106 - val_acc: 0.9270\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.15372\n",
      "Epoch 17/30\n",
      "16608/16608 [==============================] - 54s 3ms/step - loss: 0.1401 - acc: 0.9466 - val_loss: 0.2269 - val_acc: 0.9172\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.15372\n",
      "Epoch 18/30\n",
      "16608/16608 [==============================] - 59s 4ms/step - loss: 0.1414 - acc: 0.9464 - val_loss: 0.3066 - val_acc: 0.8798\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.15372\n",
      "Epoch 19/30\n",
      "16608/16608 [==============================] - 60s 4ms/step - loss: 0.1029 - acc: 0.9619 - val_loss: 0.1904 - val_acc: 0.9427\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.15372\n",
      "Epoch 20/30\n",
      "16608/16608 [==============================] - 55s 3ms/step - loss: 0.1033 - acc: 0.9681 - val_loss: 0.2004 - val_acc: 0.9376\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.15372\n",
      "Epoch 21/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.0531 - acc: 0.9827 - val_loss: 0.2024 - val_acc: 0.9458\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.15372\n",
      "Epoch 22/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.0473 - acc: 0.9854 - val_loss: 0.2162 - val_acc: 0.9425\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.15372\n",
      "Epoch 23/30\n",
      "16608/16608 [==============================] - 55s 3ms/step - loss: 0.0359 - acc: 0.9895 - val_loss: 0.2105 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.15372\n",
      "Epoch 24/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.0400 - acc: 0.9873 - val_loss: 0.2238 - val_acc: 0.9420\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.15372\n",
      "Epoch 25/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.0549 - acc: 0.9812 - val_loss: 0.2529 - val_acc: 0.9213\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.15372\n",
      "Epoch 26/30\n",
      "16608/16608 [==============================] - 58s 4ms/step - loss: 0.0916 - acc: 0.9736 - val_loss: 0.6636 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.15372\n",
      "Epoch 27/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.1229 - acc: 0.9509 - val_loss: 0.2058 - val_acc: 0.9439\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.15372\n",
      "Epoch 28/30\n",
      "16608/16608 [==============================] - 56s 3ms/step - loss: 0.0462 - acc: 0.9842 - val_loss: 0.1929 - val_acc: 0.9509\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.15372\n",
      "Epoch 29/30\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.0373 - acc: 0.9886 - val_loss: 0.2175 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.15372\n",
      "Epoch 30/30\n",
      "16608/16608 [==============================] - 55s 3ms/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.2494 - val_acc: 0.9449\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.15372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f89acede128>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_network()\n",
    "\n",
    "train_model(2, model, Adam(), 30, sequences_matrix, y, 0.2, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/20\n",
      "16608/16608 [==============================] - 59s 4ms/step - loss: 0.3428 - acc: 0.8525 - val_loss: 0.1678 - val_acc: 0.9429\n",
      "Epoch 2/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1564 - acc: 0.9461 - val_loss: 0.1852 - val_acc: 0.9364\n",
      "Epoch 3/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1217 - acc: 0.9585 - val_loss: 0.1574 - val_acc: 0.9487\n",
      "Epoch 4/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1391 - acc: 0.9535 - val_loss: 0.1793 - val_acc: 0.9372\n",
      "Epoch 5/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1008 - acc: 0.9683 - val_loss: 0.1689 - val_acc: 0.9410\n",
      "Epoch 6/20\n",
      "16608/16608 [==============================] - 62s 4ms/step - loss: 0.1229 - acc: 0.9563 - val_loss: 0.2019 - val_acc: 0.9343\n",
      "Epoch 7/20\n",
      "16608/16608 [==============================] - 64s 4ms/step - loss: 0.1262 - acc: 0.9551 - val_loss: 0.1676 - val_acc: 0.9468\n",
      "Epoch 8/20\n",
      "16608/16608 [==============================] - 69s 4ms/step - loss: 0.0805 - acc: 0.9742 - val_loss: 0.1788 - val_acc: 0.9444\n",
      "Epoch 9/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.0664 - acc: 0.9791 - val_loss: 0.1818 - val_acc: 0.9441\n",
      "Epoch 10/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.1040 - acc: 0.9603 - val_loss: 0.4086 - val_acc: 0.8023\n",
      "Epoch 11/20\n",
      "16608/16608 [==============================] - 66s 4ms/step - loss: 0.1981 - acc: 0.9202 - val_loss: 0.2112 - val_acc: 0.9343\n",
      "Epoch 12/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1181 - acc: 0.9573 - val_loss: 0.2025 - val_acc: 0.9425\n",
      "Epoch 13/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.2187 - acc: 0.9151 - val_loss: 0.4387 - val_acc: 0.7900\n",
      "Epoch 14/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1861 - acc: 0.9258 - val_loss: 0.2705 - val_acc: 0.9030\n",
      "Epoch 15/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0911 - acc: 0.9651 - val_loss: 0.1564 - val_acc: 0.9461\n",
      "Epoch 16/20\n",
      "16608/16608 [==============================] - 61s 4ms/step - loss: 0.0536 - acc: 0.9819 - val_loss: 0.1441 - val_acc: 0.9540\n",
      "Epoch 17/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0566 - acc: 0.9794 - val_loss: 0.1861 - val_acc: 0.9398\n",
      "Epoch 18/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0420 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9526\n",
      "Epoch 19/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0254 - acc: 0.9922 - val_loss: 0.1457 - val_acc: 0.9567\n",
      "Epoch 20/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0206 - acc: 0.9925 - val_loss: 0.1548 - val_acc: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42e4e7ae80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y,batch_size=128,epochs=20,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = SimpleRNN(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 64)                7360      \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 124,257\n",
      "Trainable params: 124,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.6715 - acc: 0.5984 - val_loss: 0.6128 - val_acc: 0.7250\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.61277, saving model to NNs/model-001.h5\n",
      "Epoch 2/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.5661 - acc: 0.7249 - val_loss: 0.5287 - val_acc: 0.7450\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.61277 to 0.52872, saving model to NNs/model-002.h5\n",
      "Epoch 3/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4943 - acc: 0.7620 - val_loss: 0.4776 - val_acc: 0.7597\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.52872 to 0.47757, saving model to NNs/model-003.h5\n",
      "Epoch 4/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.6548 - acc: 0.6172 - val_loss: 0.6088 - val_acc: 0.6988\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.47757\n",
      "Epoch 5/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.5700 - acc: 0.7149 - val_loss: 0.5276 - val_acc: 0.7421\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.47757\n",
      "Epoch 6/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.5377 - acc: 0.7346 - val_loss: 0.5032 - val_acc: 0.7595\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.47757\n",
      "Epoch 7/30\n",
      "16608/16608 [==============================] - 22s 1ms/step - loss: 0.5023 - acc: 0.7542 - val_loss: 0.4807 - val_acc: 0.7652\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.47757\n",
      "Epoch 8/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4626 - acc: 0.7793 - val_loss: 0.4152 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.47757 to 0.41524, saving model to NNs/model-008.h5\n",
      "Epoch 9/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.5131 - acc: 0.7633 - val_loss: 0.5042 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.41524\n",
      "Epoch 10/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.5072 - acc: 0.7641 - val_loss: 0.4990 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.41524\n",
      "Epoch 11/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4982 - acc: 0.7704 - val_loss: 0.4979 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.41524\n",
      "Epoch 12/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4931 - acc: 0.7747 - val_loss: 0.5319 - val_acc: 0.7173\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.41524\n",
      "Epoch 13/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4885 - acc: 0.7758 - val_loss: 0.5027 - val_acc: 0.7409\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.41524\n",
      "Epoch 14/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4845 - acc: 0.7819 - val_loss: 0.5094 - val_acc: 0.7339\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.41524\n",
      "Epoch 15/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4770 - acc: 0.7841 - val_loss: 0.4886 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.41524\n",
      "Epoch 16/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4684 - acc: 0.7863 - val_loss: 0.4821 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.41524\n",
      "Epoch 17/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4655 - acc: 0.7865 - val_loss: 0.5213 - val_acc: 0.7171\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.41524\n",
      "Epoch 18/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4609 - acc: 0.7879 - val_loss: 0.4973 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.41524\n",
      "Epoch 19/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4619 - acc: 0.7873 - val_loss: 0.4814 - val_acc: 0.7643\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.41524\n",
      "Epoch 20/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4541 - acc: 0.7918 - val_loss: 0.4861 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.41524\n",
      "Epoch 21/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4546 - acc: 0.7895 - val_loss: 0.5261 - val_acc: 0.7696\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.41524\n",
      "Epoch 22/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4499 - acc: 0.7916 - val_loss: 0.4945 - val_acc: 0.7310\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.41524\n",
      "Epoch 23/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4486 - acc: 0.7924 - val_loss: 0.4769 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.41524\n",
      "Epoch 24/30\n",
      "16608/16608 [==============================] - 20s 1ms/step - loss: 0.4465 - acc: 0.7920 - val_loss: 0.5017 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.41524\n",
      "Epoch 25/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4470 - acc: 0.7905 - val_loss: 0.4795 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.41524\n",
      "Epoch 26/30\n",
      "16608/16608 [==============================] - 22s 1ms/step - loss: 0.4441 - acc: 0.7906 - val_loss: 0.4800 - val_acc: 0.7660\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.41524\n",
      "Epoch 27/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4446 - acc: 0.7946 - val_loss: 0.4797 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.41524\n",
      "Epoch 28/30\n",
      "16608/16608 [==============================] - 22s 1ms/step - loss: 0.4420 - acc: 0.7967 - val_loss: 0.4828 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.41524\n",
      "Epoch 29/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4375 - acc: 0.7965 - val_loss: 0.4868 - val_acc: 0.7460\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.41524\n",
      "Epoch 30/30\n",
      "16608/16608 [==============================] - 21s 1ms/step - loss: 0.4381 - acc: 0.7949 - val_loss: 0.4897 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.41524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f897c0a7828>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_network()\n",
    "\n",
    "train_model('rnn', model, SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True), 30, sequences_matrix, y, 0.2, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-env)",
   "language": "python",
   "name": "ml-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
