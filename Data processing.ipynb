{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN, LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/train.csv')\n",
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20761, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indexes = dataset.loc[pd.isna(dataset[\"text\"]), :].index\n",
    "dataset = dataset.drop(nan_indexes)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(142 in nan_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.text\n",
    "y = dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  U.N. Secretary General Complains That The ‘Masses Have Rejected Globalism’ In Favor Of Nationalism Antonio Guterres, elected in October to take over as U.N. secretary general next year, told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas.   22, 2016 The incoming head of the United Nations warned on Tuesday that ‘losers of globalization’ in rich countries have felt ignored by establishment politicians, prompting them to turn to nationalist agendas, as in the U.S. election and Brexit referendum. “Therefore wait ye upon me, saith the LORD, until the day that I rise up to the prey: for my determination is to gather the nations, that I may assemble the kingdoms, to pour upon them mine indignation, even all my fierce anger: for all the earth shall be devoured with the fire of my jealousy.” Zephaniah 3:8 (KJV) EDITOR’S NOTE: The Bible clearly says that God’s desire is to “gather all the nations of the world together”, in order to “pour His fury” upon them. The United Nations, something unique in world history, has been created by the will of God, and things are going to end exactly how the Bible says they will end. All Muslims will be driven out of Israel as prophesied in Zechariah 14:21 (KJV), Israel will expand its borders to cover the size of the original land grant to Abraham , and Jesus will rule the world from Jerusalem . And that will be the “new” world order. Antonio Guterres, elected in October to take over as U.N. secretary general next year , told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas. Antonio Guterres formally elected as UN chief: Europe has struggled to handle a huge influx of refugees, many of whom displaced by the war in Syria. The United States has accepted only a very small number of refugees and may take in even fewer next year. “In 2016, we have witnessed a dramatic deterioration of that international protection regime (for refugees),” Guterres said. “This example started in the developed world, it started essentially in Europe, it is spreading now like a virus into other parts of the world.” Guterres, who was U.N. High Commissioner for Refugees until last year, linked the growing resistance to accepting refugees to wider concerns about globalism. “I don’t think we can look strictly at the refugee issue, I think the problem is a broader problem,” he told the conference on Europe’s refugee crisis. There was a consensus in the mid-1990s that globalization would benefit all, he said. “But a lot of people were left behind … In the developed world, (there are) those who have been losers in globalization,” he said. “The recent analysis of the rust belt in the United States, I think, is a clear demonstration of that, when we speak about the elections.” Donald Trump won this month’s election in the United States in part thanks to support from voters who have seen their jobs lost to countries with cheaper labor. “So globalization has not been as successful as we had hoped and lots of people became not only angry with it, but feeling that political establishments and international organizations are not paying attention, were not taking care (of them),” he said. This led to what he called “a kind of evolution” in which anti-establishment parties now tended to win elections and referendums tended to attract majorities against whatever was put to a vote. source SHARE THIS ARTICLE  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773.6980395934685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_article_length = 0\n",
    "\n",
    "for i in range(20761):\n",
    "    if i in nan_indexes:\n",
    "        continue\n",
    "    mean_article_length += len(X[i].split(' '))\n",
    "\n",
    "mean_article_length /= 20761\n",
    "\n",
    "mean_article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 1000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 146,337\n",
      "Trainable params: 146,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = LSTM_network()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/20\n",
      "16608/16608 [==============================] - 59s 4ms/step - loss: 0.3428 - acc: 0.8525 - val_loss: 0.1678 - val_acc: 0.9429\n",
      "Epoch 2/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1564 - acc: 0.9461 - val_loss: 0.1852 - val_acc: 0.9364\n",
      "Epoch 3/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1217 - acc: 0.9585 - val_loss: 0.1574 - val_acc: 0.9487\n",
      "Epoch 4/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1391 - acc: 0.9535 - val_loss: 0.1793 - val_acc: 0.9372\n",
      "Epoch 5/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1008 - acc: 0.9683 - val_loss: 0.1689 - val_acc: 0.9410\n",
      "Epoch 6/20\n",
      "16608/16608 [==============================] - 62s 4ms/step - loss: 0.1229 - acc: 0.9563 - val_loss: 0.2019 - val_acc: 0.9343\n",
      "Epoch 7/20\n",
      "16608/16608 [==============================] - 64s 4ms/step - loss: 0.1262 - acc: 0.9551 - val_loss: 0.1676 - val_acc: 0.9468\n",
      "Epoch 8/20\n",
      "16608/16608 [==============================] - 69s 4ms/step - loss: 0.0805 - acc: 0.9742 - val_loss: 0.1788 - val_acc: 0.9444\n",
      "Epoch 9/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.0664 - acc: 0.9791 - val_loss: 0.1818 - val_acc: 0.9441\n",
      "Epoch 10/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.1040 - acc: 0.9603 - val_loss: 0.4086 - val_acc: 0.8023\n",
      "Epoch 11/20\n",
      "16608/16608 [==============================] - 66s 4ms/step - loss: 0.1981 - acc: 0.9202 - val_loss: 0.2112 - val_acc: 0.9343\n",
      "Epoch 12/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1181 - acc: 0.9573 - val_loss: 0.2025 - val_acc: 0.9425\n",
      "Epoch 13/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.2187 - acc: 0.9151 - val_loss: 0.4387 - val_acc: 0.7900\n",
      "Epoch 14/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1861 - acc: 0.9258 - val_loss: 0.2705 - val_acc: 0.9030\n",
      "Epoch 15/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0911 - acc: 0.9651 - val_loss: 0.1564 - val_acc: 0.9461\n",
      "Epoch 16/20\n",
      "16608/16608 [==============================] - 61s 4ms/step - loss: 0.0536 - acc: 0.9819 - val_loss: 0.1441 - val_acc: 0.9540\n",
      "Epoch 17/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0566 - acc: 0.9794 - val_loss: 0.1861 - val_acc: 0.9398\n",
      "Epoch 18/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0420 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9526\n",
      "Epoch 19/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0254 - acc: 0.9922 - val_loss: 0.1457 - val_acc: 0.9567\n",
      "Epoch 20/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0206 - acc: 0.9925 - val_loss: 0.1548 - val_acc: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42e4e7ae80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y,batch_size=128,epochs=20,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = SimpleRNN(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                7360      \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 124,257\n",
      "Trainable params: 124,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/20\n",
      "16608/16608 [==============================] - 27s 2ms/step - loss: 0.4559 - acc: 0.7708 - val_loss: 0.2353 - val_acc: 0.9203\n",
      "Epoch 2/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.2019 - acc: 0.9240 - val_loss: 0.2488 - val_acc: 0.8993\n",
      "Epoch 3/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.1570 - acc: 0.9388 - val_loss: 0.3667 - val_acc: 0.8336\n",
      "Epoch 4/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0868 - acc: 0.9703 - val_loss: 0.3805 - val_acc: 0.8550\n",
      "Epoch 5/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0381 - acc: 0.9877 - val_loss: 0.3879 - val_acc: 0.8974\n",
      "Epoch 6/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0243 - acc: 0.9916 - val_loss: 0.3953 - val_acc: 0.9008\n",
      "Epoch 7/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0102 - acc: 0.9977 - val_loss: 0.3791 - val_acc: 0.9157\n",
      "Epoch 8/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.4766 - val_acc: 0.8960\n",
      "Epoch 9/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0911 - acc: 0.9678 - val_loss: 0.3184 - val_acc: 0.9066\n",
      "Epoch 10/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0266 - acc: 0.9910 - val_loss: 0.3725 - val_acc: 0.9054\n",
      "Epoch 11/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0202 - acc: 0.9930 - val_loss: 0.5114 - val_acc: 0.8779\n",
      "Epoch 12/20\n",
      "16608/16608 [==============================] - 27s 2ms/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.5811 - val_acc: 0.8526\n",
      "Epoch 13/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.5582 - val_acc: 0.8936\n",
      "Epoch 14/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.5839 - val_acc: 0.8876\n",
      "Epoch 15/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 8.0197e-04 - acc: 0.9999 - val_loss: 0.6022 - val_acc: 0.8900\n",
      "Epoch 16/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 4.0052e-04 - acc: 0.9999 - val_loss: 0.5969 - val_acc: 0.8914\n",
      "Epoch 17/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 2.6190e-04 - acc: 0.9999 - val_loss: 0.6096 - val_acc: 0.8926\n",
      "Epoch 18/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 1.8380e-04 - acc: 1.0000 - val_loss: 0.6067 - val_acc: 0.8950\n",
      "Epoch 19/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 1.5309e-04 - acc: 1.0000 - val_loss: 0.6219 - val_acc: 0.8953\n",
      "Epoch 20/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 1.3099e-04 - acc: 1.0000 - val_loss: 0.6995 - val_acc: 0.8859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42c8247048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_network()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "model.fit(sequences_matrix,y,batch_size=128,epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (my-env)",
   "language": "python",
   "name": "my-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
