{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from keras.layers import SimpleRNN, LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20800, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('data/train.csv')\n",
    "dataset.head()\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20761, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nan_indexes = dataset.loc[pd.isna(dataset[\"text\"]), :].index\n",
    "dataset = dataset.drop(nan_indexes)\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(142 in nan_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.text\n",
    "y = dataset.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  U.N. Secretary General Complains That The ‘Masses Have Rejected Globalism’ In Favor Of Nationalism Antonio Guterres, elected in October to take over as U.N. secretary general next year, told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas.   22, 2016 The incoming head of the United Nations warned on Tuesday that ‘losers of globalization’ in rich countries have felt ignored by establishment politicians, prompting them to turn to nationalist agendas, as in the U.S. election and Brexit referendum. “Therefore wait ye upon me, saith the LORD, until the day that I rise up to the prey: for my determination is to gather the nations, that I may assemble the kingdoms, to pour upon them mine indignation, even all my fierce anger: for all the earth shall be devoured with the fire of my jealousy.” Zephaniah 3:8 (KJV) EDITOR’S NOTE: The Bible clearly says that God’s desire is to “gather all the nations of the world together”, in order to “pour His fury” upon them. The United Nations, something unique in world history, has been created by the will of God, and things are going to end exactly how the Bible says they will end. All Muslims will be driven out of Israel as prophesied in Zechariah 14:21 (KJV), Israel will expand its borders to cover the size of the original land grant to Abraham , and Jesus will rule the world from Jerusalem . And that will be the “new” world order. Antonio Guterres, elected in October to take over as U.N. secretary general next year , told a conference in his native Lisbon that this trend had undermined the willingness to receive refugees in Europe this year. He said the world must re-establish international protection for refugees coming from war zones such as Syria, but it would not be easy as developed countries were turning to nationalist agendas. Antonio Guterres formally elected as UN chief: Europe has struggled to handle a huge influx of refugees, many of whom displaced by the war in Syria. The United States has accepted only a very small number of refugees and may take in even fewer next year. “In 2016, we have witnessed a dramatic deterioration of that international protection regime (for refugees),” Guterres said. “This example started in the developed world, it started essentially in Europe, it is spreading now like a virus into other parts of the world.” Guterres, who was U.N. High Commissioner for Refugees until last year, linked the growing resistance to accepting refugees to wider concerns about globalism. “I don’t think we can look strictly at the refugee issue, I think the problem is a broader problem,” he told the conference on Europe’s refugee crisis. There was a consensus in the mid-1990s that globalization would benefit all, he said. “But a lot of people were left behind … In the developed world, (there are) those who have been losers in globalization,” he said. “The recent analysis of the rust belt in the United States, I think, is a clear demonstration of that, when we speak about the elections.” Donald Trump won this month’s election in the United States in part thanks to support from voters who have seen their jobs lost to countries with cheaper labor. “So globalization has not been as successful as we had hoped and lots of people became not only angry with it, but feeling that political establishments and international organizations are not paying attention, were not taking care (of them),” he said. This led to what he called “a kind of evolution” in which anti-establishment parties now tended to win elections and referendums tended to attract majorities against whatever was put to a vote. source SHARE THIS ARTICLE  '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[143]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773.6980395934685"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_article_length = 0\n",
    "\n",
    "for i in range(20761):\n",
    "    if i in nan_indexes:\n",
    "        continue\n",
    "    mean_article_length += len(X[i].split(' '))\n",
    "\n",
    "mean_article_length /= 20761\n",
    "\n",
    "mean_article_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 2000\n",
    "max_len = 1000\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X)\n",
    "sequences = tok.texts_to_sequences(X)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import keras as K\n",
    "\n",
    "def train_model(model_id, model, optimizer, epochs, X_train, y_train, validation_split, batch_size):\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    checkpoint = ModelCheckpoint('NNs/model-{epoch:03d}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto')  \n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    tb_callback = K.callbacks.TensorBoard(log_dir='./logs/nn_' + str(model_id))\n",
    "\n",
    "    hist = model.fit(X_train, y_train, validation_split=validation_split, epochs=epochs, batch_size=batch_size, callbacks=[tb_callback, checkpoint])\n",
    "    \n",
    "    return hist\n",
    "\n",
    "#hist = train_model(...)\n",
    "#best_epoch = max(hist.history['val_loss'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 146,337\n",
      "Trainable params: 146,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/30\n",
      "16608/16608 [==============================] - 131s 8ms/step - loss: 0.3452 - acc: 0.8513 - val_loss: 0.1623 - val_acc: 0.9369\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.16228, saving model to NNs/model-001.h5\n",
      "Epoch 2/30\n",
      "16608/16608 [==============================] - 133s 8ms/step - loss: 0.1852 - acc: 0.9337 - val_loss: 0.1688 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.16228\n",
      "Epoch 3/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.1371 - acc: 0.9529 - val_loss: 0.1691 - val_acc: 0.9432\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.16228\n",
      "Epoch 4/30\n",
      "16608/16608 [==============================] - 133s 8ms/step - loss: 0.1121 - acc: 0.9620 - val_loss: 0.1419 - val_acc: 0.9523\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.16228 to 0.14193, saving model to NNs/model-004.h5\n",
      "Epoch 5/30\n",
      "16608/16608 [==============================] - 130s 8ms/step - loss: 0.1169 - acc: 0.9611 - val_loss: 0.1754 - val_acc: 0.9410\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.14193\n",
      "Epoch 6/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.0922 - acc: 0.9698 - val_loss: 0.1590 - val_acc: 0.9499\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.14193\n",
      "Epoch 7/30\n",
      "16608/16608 [==============================] - 134s 8ms/step - loss: 0.0776 - acc: 0.9745 - val_loss: 0.1874 - val_acc: 0.9311\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.14193\n",
      "Epoch 8/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.0902 - acc: 0.9696 - val_loss: 0.1736 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.14193\n",
      "Epoch 9/30\n",
      "16608/16608 [==============================] - 134s 8ms/step - loss: 0.1341 - acc: 0.9497 - val_loss: 0.2725 - val_acc: 0.8984\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.14193\n",
      "Epoch 10/30\n",
      "16608/16608 [==============================] - 127s 8ms/step - loss: 0.2323 - acc: 0.9064 - val_loss: 0.2677 - val_acc: 0.8933\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.14193\n",
      "Epoch 11/30\n",
      "16608/16608 [==============================] - 130s 8ms/step - loss: 0.1513 - acc: 0.9494 - val_loss: 0.1868 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.14193\n",
      "Epoch 12/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.0865 - acc: 0.9698 - val_loss: 0.1938 - val_acc: 0.9403\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.14193\n",
      "Epoch 13/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.3798 - acc: 0.8221 - val_loss: 0.2626 - val_acc: 0.8907\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.14193\n",
      "Epoch 14/30\n",
      "16608/16608 [==============================] - 134s 8ms/step - loss: 0.2755 - acc: 0.8786 - val_loss: 0.2344 - val_acc: 0.9138\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.14193\n",
      "Epoch 15/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.1254 - acc: 0.9522 - val_loss: 0.1807 - val_acc: 0.9326\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.14193\n",
      "Epoch 16/30\n",
      "16608/16608 [==============================] - 132s 8ms/step - loss: 0.0777 - acc: 0.9732 - val_loss: 0.1490 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.14193\n",
      "Epoch 17/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.0724 - acc: 0.9736 - val_loss: 0.1561 - val_acc: 0.9475\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.14193\n",
      "Epoch 18/30\n",
      "16608/16608 [==============================] - 127s 8ms/step - loss: 0.0553 - acc: 0.9802 - val_loss: 0.1987 - val_acc: 0.9328\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.14193\n",
      "Epoch 19/30\n",
      "16608/16608 [==============================] - 132s 8ms/step - loss: 0.0485 - acc: 0.9837 - val_loss: 0.1515 - val_acc: 0.9567\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.14193\n",
      "Epoch 20/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.0263 - acc: 0.9926 - val_loss: 0.1794 - val_acc: 0.9490\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.14193\n",
      "Epoch 21/30\n",
      "16608/16608 [==============================] - 133s 8ms/step - loss: 0.0195 - acc: 0.9947 - val_loss: 0.1781 - val_acc: 0.9516\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.14193\n",
      "Epoch 22/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.0148 - acc: 0.9961 - val_loss: 0.1758 - val_acc: 0.9576\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.14193\n",
      "Epoch 23/30\n",
      "16608/16608 [==============================] - 131s 8ms/step - loss: 0.0093 - acc: 0.9980 - val_loss: 0.2030 - val_acc: 0.9562\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.14193\n",
      "Epoch 24/30\n",
      "16608/16608 [==============================] - 129s 8ms/step - loss: 0.0089 - acc: 0.9978 - val_loss: 0.2167 - val_acc: 0.9583\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.14193\n",
      "Epoch 25/30\n",
      "16608/16608 [==============================] - 127s 8ms/step - loss: 0.0095 - acc: 0.9977 - val_loss: 0.2241 - val_acc: 0.9521\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.14193\n",
      "Epoch 26/30\n",
      "16608/16608 [==============================] - 132s 8ms/step - loss: 0.0151 - acc: 0.9955 - val_loss: 0.2197 - val_acc: 0.9494\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.14193\n",
      "Epoch 27/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.0098 - acc: 0.9974 - val_loss: 0.2317 - val_acc: 0.9485\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.14193\n",
      "Epoch 28/30\n",
      "16608/16608 [==============================] - 133s 8ms/step - loss: 0.0119 - acc: 0.9957 - val_loss: 0.2321 - val_acc: 0.9530\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.14193\n",
      "Epoch 29/30\n",
      "16608/16608 [==============================] - 127s 8ms/step - loss: 0.0050 - acc: 0.9989 - val_loss: 0.2656 - val_acc: 0.9504\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.14193\n",
      "Epoch 30/30\n",
      "16608/16608 [==============================] - 128s 8ms/step - loss: 0.0138 - acc: 0.9955 - val_loss: 0.2589 - val_acc: 0.9497\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.14193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf1bd57cc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTM_network()\n",
    "\n",
    "train_model(1, model, Adam(), 30, sequences_matrix, y, 0.2, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/20\n",
      "16608/16608 [==============================] - 59s 4ms/step - loss: 0.3428 - acc: 0.8525 - val_loss: 0.1678 - val_acc: 0.9429\n",
      "Epoch 2/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1564 - acc: 0.9461 - val_loss: 0.1852 - val_acc: 0.9364\n",
      "Epoch 3/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1217 - acc: 0.9585 - val_loss: 0.1574 - val_acc: 0.9487\n",
      "Epoch 4/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1391 - acc: 0.9535 - val_loss: 0.1793 - val_acc: 0.9372\n",
      "Epoch 5/20\n",
      "16608/16608 [==============================] - 58s 3ms/step - loss: 0.1008 - acc: 0.9683 - val_loss: 0.1689 - val_acc: 0.9410\n",
      "Epoch 6/20\n",
      "16608/16608 [==============================] - 62s 4ms/step - loss: 0.1229 - acc: 0.9563 - val_loss: 0.2019 - val_acc: 0.9343\n",
      "Epoch 7/20\n",
      "16608/16608 [==============================] - 64s 4ms/step - loss: 0.1262 - acc: 0.9551 - val_loss: 0.1676 - val_acc: 0.9468\n",
      "Epoch 8/20\n",
      "16608/16608 [==============================] - 69s 4ms/step - loss: 0.0805 - acc: 0.9742 - val_loss: 0.1788 - val_acc: 0.9444\n",
      "Epoch 9/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.0664 - acc: 0.9791 - val_loss: 0.1818 - val_acc: 0.9441\n",
      "Epoch 10/20\n",
      "16608/16608 [==============================] - 68s 4ms/step - loss: 0.1040 - acc: 0.9603 - val_loss: 0.4086 - val_acc: 0.8023\n",
      "Epoch 11/20\n",
      "16608/16608 [==============================] - 66s 4ms/step - loss: 0.1981 - acc: 0.9202 - val_loss: 0.2112 - val_acc: 0.9343\n",
      "Epoch 12/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1181 - acc: 0.9573 - val_loss: 0.2025 - val_acc: 0.9425\n",
      "Epoch 13/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.2187 - acc: 0.9151 - val_loss: 0.4387 - val_acc: 0.7900\n",
      "Epoch 14/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.1861 - acc: 0.9258 - val_loss: 0.2705 - val_acc: 0.9030\n",
      "Epoch 15/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0911 - acc: 0.9651 - val_loss: 0.1564 - val_acc: 0.9461\n",
      "Epoch 16/20\n",
      "16608/16608 [==============================] - 61s 4ms/step - loss: 0.0536 - acc: 0.9819 - val_loss: 0.1441 - val_acc: 0.9540\n",
      "Epoch 17/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0566 - acc: 0.9794 - val_loss: 0.1861 - val_acc: 0.9398\n",
      "Epoch 18/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0420 - acc: 0.9866 - val_loss: 0.1569 - val_acc: 0.9526\n",
      "Epoch 19/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0254 - acc: 0.9922 - val_loss: 0.1457 - val_acc: 0.9567\n",
      "Epoch 20/20\n",
      "16608/16608 [==============================] - 57s 3ms/step - loss: 0.0206 - acc: 0.9925 - val_loss: 0.1548 - val_acc: 0.9559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42e4e7ae80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y,batch_size=128,epochs=20,\n",
    "          validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN_network():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = SimpleRNN(64)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.2)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 1000, 50)          100000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 64)                7360      \n",
      "_________________________________________________________________\n",
      "FC1 (Dense)                  (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "out_layer (Dense)            (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 124,257\n",
      "Trainable params: 124,257\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 16608 samples, validate on 4153 samples\n",
      "Epoch 1/20\n",
      "16608/16608 [==============================] - 27s 2ms/step - loss: 0.4559 - acc: 0.7708 - val_loss: 0.2353 - val_acc: 0.9203\n",
      "Epoch 2/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.2019 - acc: 0.9240 - val_loss: 0.2488 - val_acc: 0.8993\n",
      "Epoch 3/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.1570 - acc: 0.9388 - val_loss: 0.3667 - val_acc: 0.8336\n",
      "Epoch 4/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0868 - acc: 0.9703 - val_loss: 0.3805 - val_acc: 0.8550\n",
      "Epoch 5/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0381 - acc: 0.9877 - val_loss: 0.3879 - val_acc: 0.8974\n",
      "Epoch 6/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0243 - acc: 0.9916 - val_loss: 0.3953 - val_acc: 0.9008\n",
      "Epoch 7/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0102 - acc: 0.9977 - val_loss: 0.3791 - val_acc: 0.9157\n",
      "Epoch 8/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0071 - acc: 0.9978 - val_loss: 0.4766 - val_acc: 0.8960\n",
      "Epoch 9/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0911 - acc: 0.9678 - val_loss: 0.3184 - val_acc: 0.9066\n",
      "Epoch 10/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0266 - acc: 0.9910 - val_loss: 0.3725 - val_acc: 0.9054\n",
      "Epoch 11/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0202 - acc: 0.9930 - val_loss: 0.5114 - val_acc: 0.8779\n",
      "Epoch 12/20\n",
      "16608/16608 [==============================] - 27s 2ms/step - loss: 0.0170 - acc: 0.9945 - val_loss: 0.5811 - val_acc: 0.8526\n",
      "Epoch 13/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 0.0099 - acc: 0.9970 - val_loss: 0.5582 - val_acc: 0.8936\n",
      "Epoch 14/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 0.0015 - acc: 0.9997 - val_loss: 0.5839 - val_acc: 0.8876\n",
      "Epoch 15/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 8.0197e-04 - acc: 0.9999 - val_loss: 0.6022 - val_acc: 0.8900\n",
      "Epoch 16/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 4.0052e-04 - acc: 0.9999 - val_loss: 0.5969 - val_acc: 0.8914\n",
      "Epoch 17/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 2.6190e-04 - acc: 0.9999 - val_loss: 0.6096 - val_acc: 0.8926\n",
      "Epoch 18/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 1.8380e-04 - acc: 1.0000 - val_loss: 0.6067 - val_acc: 0.8950\n",
      "Epoch 19/20\n",
      "16608/16608 [==============================] - 26s 2ms/step - loss: 1.5309e-04 - acc: 1.0000 - val_loss: 0.6219 - val_acc: 0.8953\n",
      "Epoch 20/20\n",
      "16608/16608 [==============================] - 25s 2ms/step - loss: 1.3099e-04 - acc: 1.0000 - val_loss: 0.6995 - val_acc: 0.8859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f42c8247048>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN_network()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
    "model.fit(sequences_matrix,y,batch_size=128,epochs=20,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
